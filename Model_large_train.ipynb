{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af71ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "train_path = r\"D:\\Amazon ML\\student_resource\\dataset\\large_train.csv\"\n",
    "test_path = r\"D:\\Amazon ML\\student_resource\\dataset\\test.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa239b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "CUDA device name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af6f4862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rames\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23cf9df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119999, 6) (30000, 6)\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c2e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log1p(train_data[\"price\"].values)  # log(price + 1)\n",
    "y_val = np.log1p(val_data[\"price\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d2d5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3d42fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c289d2fef44e57abe4e7fb7374096f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating validation embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dec5e47747f4144b8a2d4d9f9cbf695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape: (119999, 768)\n",
      "Validation embeddings shape: (30000, 768)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model on GPU\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device='cuda')\n",
    "\n",
    "# Encode train and validation texts\n",
    "train_texts = train_data[\"clean_text\"].tolist()\n",
    "val_texts = val_data[\"clean_text\"].tolist()\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Generating train embeddings...\")\n",
    "train_embeddings = model.encode(train_texts, batch_size=64, show_progress_bar=True, device='cuda')\n",
    "\n",
    "print(\"Generating validation embeddings...\")\n",
    "val_embeddings = model.encode(val_texts, batch_size=64, show_progress_bar=True, device='cuda')\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train_bert = np.array(train_embeddings)\n",
    "X_val_bert = np.array(val_embeddings)\n",
    "\n",
    "print(\"Train embeddings shape:\", X_train_bert.shape)\n",
    "print(\"Validation embeddings shape:\", X_val_bert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68664f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained EfficientNet-B3\n",
    "efficientnet = models.efficientnet_b3(weights='IMAGENET1K_V1')  # new API\n",
    "efficientnet = efficientnet.to(device)\n",
    "efficientnet.eval()  # set to evaluation mode\n",
    "\n",
    "# Remove classifier to get embeddings (keep avgpool)\n",
    "feature_extractor = torch.nn.Sequential(\n",
    "    efficientnet.features,\n",
    "    efficientnet.avgpool\n",
    ")\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abb30ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [02:54<00:00, 2.03MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CLIP model\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f3b461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_clip_image_embeddings(img_paths, batch_size=16):\n",
    "    all_embeds = []\n",
    "\n",
    "    for i in tqdm(range(0, len(img_paths), batch_size), desc=\"Extracting CLIP embeddings\"):\n",
    "        batch_imgs = []\n",
    "\n",
    "        for p in img_paths[i:i+batch_size]:\n",
    "            try:\n",
    "                with Image.open(p).convert('RGB') as img:\n",
    "                    batch_imgs.append(preprocess(img))\n",
    "            except Exception:\n",
    "                batch_imgs.append(torch.zeros(3, 224, 224))  # fallback\n",
    "\n",
    "        batch_tensor = torch.stack(batch_imgs).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feats = clip_model.encode_image(batch_tensor)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)  # normalize\n",
    "            all_embeds.append(feats.cpu().numpy())\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return np.concatenate(all_embeds, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69089c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting CLIP embeddings: 100%|██████████| 2500/2500 [1:58:35<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting validation image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting CLIP embeddings:  33%|███▎      | 206/625 [10:06<20:33,  2.94s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m X_train_img \u001b[38;5;241m=\u001b[39m get_clip_image_embeddings(train_img_paths, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting validation image embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m X_val_img \u001b[38;5;241m=\u001b[39m \u001b[43mget_clip_image_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_img_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# --- Verify shapes ---\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain image embeddings shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_img\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m, in \u001b[0;36mget_clip_image_embeddings\u001b[1;34m(img_paths, batch_size)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m img_paths[i:i\u001b[38;5;241m+\u001b[39mbatch_size]:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img:\n\u001b[0;32m     15\u001b[0m             batch_imgs\u001b[38;5;241m.\u001b[39mappend(preprocess(img))\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rames\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    991\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rames\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "train_img_folder = r\"D:\\Amazon ML\\student_resource\\dataset\\images\\train\"\n",
    "val_img_folder = r\"D:\\Amazon ML\\student_resource\\dataset\\images\\train\"  # same folder\n",
    "\n",
    "# Build image paths\n",
    "train_img_paths = [\n",
    "    os.path.join(train_img_folder, f\"{sid}.jpg\") for sid in train_data[\"sample_id\"]\n",
    "]\n",
    "val_img_paths = [\n",
    "    os.path.join(val_img_folder, f\"{sid}.jpg\") for sid in val_data[\"sample_id\"]\n",
    "]\n",
    "\n",
    "# --- Extract embeddings ---\n",
    "print(\"Extracting train image embeddings...\")\n",
    "X_train_img = get_clip_image_embeddings(train_img_paths, batch_size=48)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "432a256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting validation image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting CLIP embeddings: 100%|██████████| 625/625 [29:32<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image embeddings shape: (119999, 512)\n",
      "Validation image embeddings shape: (30000, 512)\n",
      "✅ Image embeddings saved successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting validation image embeddings...\")\n",
    "X_val_img = get_clip_image_embeddings(val_img_paths, batch_size=48)\n",
    "\n",
    "# --- Verify shapes ---\n",
    "print(f\"Train image embeddings shape: {X_train_img.shape}\")\n",
    "print(f\"Validation image embeddings shape: {X_val_img.shape}\")\n",
    "\n",
    "# --- Save embeddings ---\n",
    "os.makedirs(r\"D:\\Amazon ML\\student_resource\\dataset\", exist_ok=True)\n",
    "np.save(r\"D:\\Amazon ML\\student_resource\\dataset\\train_image_embeds.npy\", X_train_img)\n",
    "np.save(r\"D:\\Amazon ML\\student_resource\\dataset\\val_image_embeds.npy\", X_val_img)\n",
    "\n",
    "print(\"✅ Image embeddings saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2031d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = np.hstack([X_train_bert, X_train_img, train_data[\"item_quantity\"].values.reshape(-1,1)])\n",
    "X_val_final = np.hstack([X_val_bert, X_val_img, val_data[\"item_quantity\"].values.reshape(-1,1)])\n",
    "# Targets (log-transform)\n",
    "y_train_log = np.log1p(train_data[\"price\"].values)\n",
    "y_val_log = np.log1p(val_data[\"price\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a987656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 326565\n",
      "[LightGBM] [Info] Number of data points in the train set: 119999, number of used features: 1281\n",
      "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics, Vendor: Intel(R) Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1281 dense feature groups (146.94 MB) transferred to GPU in 0.064612 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 2.739347\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's l1: 0.476291\tvalid_0's l2: 0.412161\n",
      "[100]\tvalid_0's l1: 0.443534\tvalid_0's l2: 0.369023\n",
      "[150]\tvalid_0's l1: 0.42788\tvalid_0's l2: 0.348667\n",
      "[200]\tvalid_0's l1: 0.418607\tvalid_0's l2: 0.336315\n",
      "[250]\tvalid_0's l1: 0.412054\tvalid_0's l2: 0.327674\n",
      "[300]\tvalid_0's l1: 0.407085\tvalid_0's l2: 0.321022\n",
      "[350]\tvalid_0's l1: 0.403013\tvalid_0's l2: 0.315766\n",
      "[400]\tvalid_0's l1: 0.399705\tvalid_0's l2: 0.311406\n",
      "[450]\tvalid_0's l1: 0.396948\tvalid_0's l2: 0.307851\n",
      "[500]\tvalid_0's l1: 0.39466\tvalid_0's l2: 0.304913\n",
      "[550]\tvalid_0's l1: 0.392676\tvalid_0's l2: 0.302317\n",
      "[600]\tvalid_0's l1: 0.391118\tvalid_0's l2: 0.300308\n",
      "[650]\tvalid_0's l1: 0.389703\tvalid_0's l2: 0.298476\n",
      "[700]\tvalid_0's l1: 0.388565\tvalid_0's l2: 0.29701\n",
      "[750]\tvalid_0's l1: 0.387611\tvalid_0's l2: 0.295769\n",
      "[800]\tvalid_0's l1: 0.386694\tvalid_0's l2: 0.294569\n",
      "[850]\tvalid_0's l1: 0.385893\tvalid_0's l2: 0.293554\n",
      "[900]\tvalid_0's l1: 0.385209\tvalid_0's l2: 0.292633\n",
      "[950]\tvalid_0's l1: 0.384516\tvalid_0's l2: 0.291784\n",
      "[1000]\tvalid_0's l1: 0.383883\tvalid_0's l2: 0.290977\n",
      "[1050]\tvalid_0's l1: 0.383287\tvalid_0's l2: 0.290222\n",
      "[1100]\tvalid_0's l1: 0.382655\tvalid_0's l2: 0.289495\n",
      "[1150]\tvalid_0's l1: 0.382126\tvalid_0's l2: 0.288821\n",
      "[1200]\tvalid_0's l1: 0.381552\tvalid_0's l2: 0.288176\n",
      "[1250]\tvalid_0's l1: 0.380996\tvalid_0's l2: 0.287527\n",
      "[1300]\tvalid_0's l1: 0.380616\tvalid_0's l2: 0.287035\n",
      "[1350]\tvalid_0's l1: 0.380137\tvalid_0's l2: 0.286508\n",
      "[1400]\tvalid_0's l1: 0.379729\tvalid_0's l2: 0.286021\n",
      "[1450]\tvalid_0's l1: 0.379382\tvalid_0's l2: 0.285631\n",
      "[1500]\tvalid_0's l1: 0.379086\tvalid_0's l2: 0.285273\n",
      "[1550]\tvalid_0's l1: 0.378776\tvalid_0's l2: 0.284942\n",
      "[1600]\tvalid_0's l1: 0.378419\tvalid_0's l2: 0.284543\n",
      "[1650]\tvalid_0's l1: 0.37804\tvalid_0's l2: 0.284101\n",
      "[1700]\tvalid_0's l1: 0.377722\tvalid_0's l2: 0.283756\n",
      "[1750]\tvalid_0's l1: 0.377422\tvalid_0's l2: 0.283381\n",
      "[1800]\tvalid_0's l1: 0.377124\tvalid_0's l2: 0.283006\n",
      "[1850]\tvalid_0's l1: 0.376847\tvalid_0's l2: 0.28268\n",
      "[1900]\tvalid_0's l1: 0.376486\tvalid_0's l2: 0.28227\n",
      "[1950]\tvalid_0's l1: 0.376185\tvalid_0's l2: 0.281934\n",
      "[2000]\tvalid_0's l1: 0.375875\tvalid_0's l2: 0.281624\n",
      "[2050]\tvalid_0's l1: 0.375618\tvalid_0's l2: 0.281354\n",
      "[2100]\tvalid_0's l1: 0.375365\tvalid_0's l2: 0.281058\n",
      "[2150]\tvalid_0's l1: 0.375151\tvalid_0's l2: 0.280765\n",
      "[2200]\tvalid_0's l1: 0.374945\tvalid_0's l2: 0.280552\n",
      "[2250]\tvalid_0's l1: 0.374699\tvalid_0's l2: 0.280271\n",
      "[2300]\tvalid_0's l1: 0.374429\tvalid_0's l2: 0.279978\n",
      "[2350]\tvalid_0's l1: 0.374256\tvalid_0's l2: 0.27979\n",
      "[2400]\tvalid_0's l1: 0.374092\tvalid_0's l2: 0.279609\n",
      "[2450]\tvalid_0's l1: 0.373922\tvalid_0's l2: 0.279376\n",
      "[2500]\tvalid_0's l1: 0.373702\tvalid_0's l2: 0.27919\n",
      "[2550]\tvalid_0's l1: 0.373558\tvalid_0's l2: 0.278951\n",
      "[2600]\tvalid_0's l1: 0.373365\tvalid_0's l2: 0.278725\n",
      "[2650]\tvalid_0's l1: 0.373152\tvalid_0's l2: 0.278534\n",
      "[2700]\tvalid_0's l1: 0.372948\tvalid_0's l2: 0.278329\n",
      "[2750]\tvalid_0's l1: 0.372746\tvalid_0's l2: 0.278153\n",
      "[2800]\tvalid_0's l1: 0.372612\tvalid_0's l2: 0.277985\n",
      "[2850]\tvalid_0's l1: 0.37244\tvalid_0's l2: 0.277822\n",
      "[2900]\tvalid_0's l1: 0.372274\tvalid_0's l2: 0.277643\n",
      "[2950]\tvalid_0's l1: 0.372149\tvalid_0's l2: 0.277476\n",
      "[3000]\tvalid_0's l1: 0.372056\tvalid_0's l2: 0.27733\n",
      "[3050]\tvalid_0's l1: 0.371904\tvalid_0's l2: 0.277151\n",
      "[3100]\tvalid_0's l1: 0.371721\tvalid_0's l2: 0.27696\n",
      "[3150]\tvalid_0's l1: 0.371579\tvalid_0's l2: 0.276813\n",
      "[3200]\tvalid_0's l1: 0.371442\tvalid_0's l2: 0.276683\n",
      "[3250]\tvalid_0's l1: 0.371312\tvalid_0's l2: 0.276546\n",
      "[3300]\tvalid_0's l1: 0.371176\tvalid_0's l2: 0.276409\n",
      "[3350]\tvalid_0's l1: 0.371051\tvalid_0's l2: 0.276282\n",
      "[3400]\tvalid_0's l1: 0.370947\tvalid_0's l2: 0.276186\n",
      "[3450]\tvalid_0's l1: 0.370835\tvalid_0's l2: 0.276085\n",
      "[3500]\tvalid_0's l1: 0.370708\tvalid_0's l2: 0.275991\n",
      "[3550]\tvalid_0's l1: 0.370593\tvalid_0's l2: 0.275857\n",
      "[3600]\tvalid_0's l1: 0.37049\tvalid_0's l2: 0.275755\n",
      "[3650]\tvalid_0's l1: 0.37038\tvalid_0's l2: 0.275658\n",
      "[3700]\tvalid_0's l1: 0.370336\tvalid_0's l2: 0.275614\n",
      "[3750]\tvalid_0's l1: 0.370217\tvalid_0's l2: 0.2755\n",
      "[3800]\tvalid_0's l1: 0.37014\tvalid_0's l2: 0.275429\n",
      "[3850]\tvalid_0's l1: 0.370041\tvalid_0's l2: 0.275318\n",
      "[3900]\tvalid_0's l1: 0.369949\tvalid_0's l2: 0.275215\n",
      "[3950]\tvalid_0's l1: 0.36985\tvalid_0's l2: 0.2751\n",
      "[4000]\tvalid_0's l1: 0.36978\tvalid_0's l2: 0.275005\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3997]\tvalid_0's l1: 0.36978\tvalid_0's l2: 0.275008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(colsample_bytree=0.8, device=&#x27;gpu&#x27;, learning_rate=0.03,\n",
       "              max_depth=12, n_estimators=4000, num_leaves=64,\n",
       "              objective=&#x27;regression&#x27;, random_state=42, subsample=0.8)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LGBMRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMRegressor(colsample_bytree=0.8, device=&#x27;gpu&#x27;, learning_rate=0.03,\n",
       "              max_depth=12, n_estimators=4000, num_leaves=64,\n",
       "              objective=&#x27;regression&#x27;, random_state=42, subsample=0.8)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(colsample_bytree=0.8, device='gpu', learning_rate=0.03,\n",
       "              max_depth=12, n_estimators=4000, num_leaves=64,\n",
       "              objective='regression', random_state=42, subsample=0.8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "\n",
    "model = LGBMRegressor(\n",
    "    objective='regression',\n",
    "    boosting_type='gbdt',\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=64,\n",
    "    max_depth=12,\n",
    "    n_estimators=4000,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    device='gpu'\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_final, y_train_log,\n",
    "    eval_set=[(X_val_final, y_val_log)],\n",
    "    eval_metric='mae',\n",
    "    callbacks=[\n",
    "        early_stopping(stopping_rounds=50),\n",
    "        log_evaluation(period=50)  # prints progress every 50 rounds\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14deffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal LightGBM SMAPE on validation: 37.85\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "val_preds_log = model.predict(X_val_final)\n",
    "val_preds = np.expm1(val_preds_log)  # back to original price scale\n",
    "val_true = val_data[\"price\"].values\n",
    "\n",
    "# Compute SMAPE\n",
    "smape = np.mean(np.abs(val_preds - val_true) / ((np.abs(val_preds) + np.abs(val_true))/2)*100) \n",
    "print(f\"Multimodal LightGBM SMAPE on validation: {smape:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f17d6767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multimodal LightGBM model saved\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model, r\"D:\\Amazon ML\\student_resource\\dataset\\lgbm_multimodal_model_1.pkl\")\n",
    "print(\"✅ Multimodal LightGBM model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb55053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,-]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "def extract_quantity(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 1\n",
    "    patterns = [\n",
    "        r\"pack of (\\d+)\",\n",
    "        r\"set of (\\d+)\",\n",
    "        r\"(\\d+)\\s?pcs?\",\n",
    "        r\"(\\d+)\\s?x\",\n",
    "        r\"(\\d+)\\s?count\",\n",
    "        r\"(\\d+)\\s?piece\"\n",
    "    ]\n",
    "    for p in patterns:\n",
    "        m = re.search(p, text)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c39bef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 3093.26it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 11398.49it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21d01881f014c61822834bfd3c6b8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting CLIP embeddings: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load sample test and output\n",
    "sample_test = pd.read_csv(r\"D:\\Amazon ML\\student_resource\\dataset\\sample_test.csv\")\n",
    "sample_test_out = pd.read_csv(r\"D:\\Amazon ML\\student_resource\\dataset\\sample_test_out.csv\")\n",
    "\n",
    "\n",
    "# Apply to sample_test\n",
    "sample_test[\"clean_text\"] = sample_test[\"catalog_content\"].progress_apply(clean_text)\n",
    "sample_test[\"item_quantity\"] = sample_test[\"clean_text\"].progress_apply(extract_quantity)\n",
    "\n",
    "# Extract features\n",
    "# Text embeddings\n",
    "sample_test_texts = sample_test[\"clean_text\"].tolist()\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a sentence transformer model\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2', device='cuda')\n",
    "\n",
    "# Encode text\n",
    "X_sample_text = embedding_model.encode(sample_test_texts, batch_size=64, show_progress_bar=True, device='cuda')\n",
    "\n",
    "\n",
    "# Image embeddings\n",
    "sample_img_folder = r\"D:\\Amazon ML\\student_resource\\dataset\\images\\sample_test\"\n",
    "sample_img_paths = [os.path.join(sample_img_folder, f\"{sid}.jpg\") for sid in sample_test[\"sample_id\"]]\n",
    "X_sample_img = get_clip_image_embeddings(sample_img_paths, batch_size=32)\n",
    "\n",
    "# Combine embeddings + numeric\n",
    "X_sample_final = np.hstack([\n",
    "    X_sample_text,\n",
    "    X_sample_img,\n",
    "    sample_test[\"item_quantity\"].values.reshape(-1,1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc9c29a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load trained multimodal LightGBM\n",
    "model = joblib.load(r\"D:\\Amazon ML\\student_resource\\dataset\\lgbm_multimodal_model_1.pkl\")\n",
    "\n",
    "# Predict (log scale → back to original)\n",
    "sample_preds_log = model.predict(X_sample_final)\n",
    "sample_preds = np.expm1(sample_preds_log)  # this is your actual predicted price array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab3a9bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal LightGBM SMAPE on sample_test: 108.01%\n"
     ]
    }
   ],
   "source": [
    "# True prices\n",
    "y_true = sample_test_out[\"price\"].values\n",
    "\n",
    "# Predicted prices\n",
    "y_pred = sample_preds  # from previous step\n",
    "\n",
    "# Compute SMAPE\n",
    "smape = np.mean(np.abs(y_pred - y_true) / ((np.abs(y_pred) + np.abs(y_true))/2)*100) \n",
    "print(f\"Multimodal LightGBM SMAPE on sample_test: {smape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f572302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:03<00:00, 20066.88it/s]\n",
      "100%|██████████| 75000/75000 [00:02<00:00, 26595.57it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e0cb79a4de41cfb5721a92bf87e002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting CLIP embeddings: 100%|██████████| 2344/2344 [1:16:52<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load sample test and output\n",
    "test_clean = pd.read_csv(r\"D:\\Amazon ML\\student_resource\\dataset\\test_clean.csv\")\n",
    "\n",
    "\n",
    "# Apply to sample_test\n",
    "test_clean[\"clean_text\"] = test_clean[\"catalog_content\"].progress_apply(clean_text)\n",
    "test_clean[\"item_quantity\"] = test_clean[\"clean_text\"].progress_apply(extract_quantity)\n",
    "\n",
    "# Extract features\n",
    "# Text embeddings\n",
    "test_clean_texts = test_clean[\"clean_text\"].tolist()\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a sentence transformer model\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2', device='cuda')\n",
    "\n",
    "# Encode text\n",
    "X_sample_text = embedding_model.encode(test_clean_texts, batch_size=64, show_progress_bar=True, device='cuda')\n",
    "\n",
    "\n",
    "# Image embeddings\n",
    "test_img_folder = r\"D:\\Amazon ML\\student_resource\\dataset\\images\\test\"\n",
    "test_img_paths = [os.path.join(test_img_folder, f\"{sid}.jpg\") for sid in test_clean[\"sample_id\"]]\n",
    "X_sample_img = get_clip_image_embeddings(test_img_paths, batch_size=32)\n",
    "\n",
    "# Combine embeddings + numeric\n",
    "X_sample_final = np.hstack([\n",
    "    X_sample_text,\n",
    "    X_sample_img,\n",
    "    test_clean[\"item_quantity\"].values.reshape(-1,1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65a62e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions to: D:\\Amazon ML\\student_resource\\dataset\\test_out_2.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict (log scale → back to original)\n",
    "sample_preds_log = model.predict(X_sample_final)\n",
    "sample_preds = np.expm1(sample_preds_log)  # this is your actual predicted price array\n",
    "# Create DataFrame for submission\n",
    "submission = pd.DataFrame({\n",
    "    \"sample_id\": test_clean[\"sample_id\"],\n",
    "    \"price\": sample_preds\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_path = r\"D:\\Amazon ML\\student_resource\\dataset\\test_out_2.csv\"\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Saved predictions to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
